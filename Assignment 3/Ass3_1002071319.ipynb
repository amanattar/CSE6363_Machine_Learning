{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, criterion='gini', max_depth=10, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def gini_criterion(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        prob = counts / len(y)\n",
    "        return 1 - np.sum(prob ** 2)\n",
    "\n",
    "    def entropy_criterion(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        prob = counts / len(y)\n",
    "        return -np.sum(prob * np.log2(prob))\n",
    "\n",
    "    def misclassification_rate_criterion(self, y):\n",
    "        _, counts = np.unique(y, return_counts=True)\n",
    "        prob = counts / len(y)\n",
    "        return 1 - np.max(prob)\n",
    "\n",
    "    def split(self, X, y, feature_index, threshold):\n",
    "        l_mask = X[:, feature_index] <= threshold\n",
    "        r_mask = ~l_mask\n",
    "        return X[l_mask], X[r_mask], y[l_mask], y[r_mask]\n",
    "\n",
    "    def build_tree(self, X, y, depth):\n",
    "        no_samples, no_features = X.shape\n",
    "        no_classes = len(np.unique(y))\n",
    "        if no_classes == 1:\n",
    "            return y[0]\n",
    "        if depth == self.max_depth:\n",
    "            return np.argmax(np.bincount(y))\n",
    "        if no_samples < self.min_samples_split:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        best_criterion_value = -1\n",
    "        best_criterion = None\n",
    "        best_feature_index = None\n",
    "        best_threshold = None\n",
    "\n",
    "        for feature_index in range(no_features):\n",
    "            unique_values = np.unique(X[:, feature_index])\n",
    "            thresholds = (unique_values[:-1] + unique_values[1:]) / 2\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = self.split(X, y, feature_index, threshold)\n",
    "                if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                if self.criterion == 'gini':\n",
    "                    criterion_value = self.gini_criterion(y_left) * len(y_left) / no_samples + \\\n",
    "                                      self.gini_criterion(y_right) * len(y_right) / no_samples\n",
    "                elif self.criterion == 'entropy':\n",
    "                    criterion_value = self.entropy_criterion(y_left) * len(y_left) / no_samples + \\\n",
    "                                      self.entropy_criterion(y_right) * len(y_right) / no_samples\n",
    "                elif self.criterion == 'misclassification':\n",
    "                    criterion_value = self.misclassification_rate_criterion(y_left) * len(y_left) / no_samples + \\\n",
    "                                      self.misclassification_rate_criterion(y_right) * len(y_right) / no_samples\n",
    "\n",
    "                if criterion_value > best_criterion_value:\n",
    "                    best_criterion_value = criterion_value\n",
    "                    best_criterion = (feature_index, threshold)\n",
    "                    best_feature_index = feature_index\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        if best_criterion_value == -1:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        X_left, X_right, y_left, y_right = self.split(X, y, best_feature_index, best_threshold)\n",
    "\n",
    "        left_subtree = self.build_tree(X_left, y_left, depth + 1)\n",
    "        right_subtree = self.build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return (best_feature_index, best_threshold, left_subtree, right_subtree)\n",
    "\n",
    "    def predict_tree(self, x, node):\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "        \n",
    "        feature_index, threshold, left_subtree, right_subtree = node\n",
    "\n",
    "        if x[feature_index] <= threshold:\n",
    "            return self.predict_tree(x, left_subtree)\n",
    "        else:\n",
    "            return self.predict_tree(x, right_subtree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing for Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from IPython.display import Image as PImage\n",
    "from subprocess import check_call\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = train.copy()\n",
    "\n",
    "full_data = [train, test]\n",
    "\n",
    "\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "for dataset in full_data:\n",
    "\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree with Gini: 73.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Survived'].values\n",
    "X = train.drop('Survived', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6739)\n",
    "\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='gini', max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
    "Decision_Tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = Decision_Tree.predict(X_test)\n",
    "\n",
    "Accuracy_DT_gini = accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy for Decision Tree with Gini: {:.2f}%'.format(Accuracy_DT_gini*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree with Entropy: 70.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Survived'].values\n",
    "X = train.drop('Survived', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6739)\n",
    "\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='entropy', max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
    "Decision_Tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = Decision_Tree.predict(X_test)\n",
    "\n",
    "Accuracy_DT_entropy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy for Decision Tree with Entropy: {:.2f}%'.format(Accuracy_DT_entropy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree with Misclassification: 63.69%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Survived'].values\n",
    "X = train.drop('Survived', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6739)\n",
    "\n",
    "Decision_Tree = DecisionTreeClassifier(criterion='misclassification', max_depth=10, min_samples_split=2, min_samples_leaf=1)\n",
    "Decision_Tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = Decision_Tree.predict(X_test)\n",
    "\n",
    "Accuracy_DT_misclassification = accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy for Decision Tree with Misclassification: {:.2f}%'.format(Accuracy_DT_misclassification*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class RandomForestClassifier:\n",
    "    def __init__(self, main_classifier, no_trees, min_features):\n",
    "        self.main_classifier = main_classifier\n",
    "        self.no_trees = no_trees\n",
    "        self.min_features = min_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_features = X.shape[1]\n",
    "\n",
    "        for _ in range(self.no_trees):\n",
    "            idx = np.random.choice(len(y), len(y), replace=True)\n",
    "            X_subset, y_subset = X[idx], y[idx]\n",
    "\n",
    "            num_selected_features = np.random.randint(self.min_features, num_features + 1)\n",
    "            selected_feature_indices = np.random.choice(num_features, num_selected_features, replace=False)\n",
    "\n",
    "            tree = self.main_classifier(max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "            tree.fit(X_subset[:, selected_feature_indices], y_subset)\n",
    "            \n",
    "            self.trees.append((selected_feature_indices, tree))\n",
    "\n",
    "    def predict(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        num_classes = len(np.unique(y))\n",
    "\n",
    "        predictions = np.zeros((num_samples, num_classes))\n",
    "\n",
    "        for features_indices, tree in self.trees:\n",
    "            tree_predictions = tree.predict(X[:, features_indices])\n",
    "            predictions[np.arange(num_samples), tree_predictions] += 1\n",
    "\n",
    "        final_predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "        return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Random Forest:79.33%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Survived'].values\n",
    "X = train.drop('Survived', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6739)\n",
    "\n",
    "Random_Forest = RandomForestClassifier(main_classifier=DecisionTreeClassifier, no_trees=10, min_features=6)\n",
    "Random_Forest.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = Random_Forest.predict(X_test)\n",
    "\n",
    "Accuracy_RF = accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy for Random Forest:{:.2f}%'.format(Accuracy_RF*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, weak_learner, num_learners, learning_rate):\n",
    "        self.weak_learner = weak_learner\n",
    "        self.num_learners = num_learners\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples = X.shape[0]\n",
    "        weights = np.ones(num_samples) / num_samples \n",
    "\n",
    "        self.m_arr = []  \n",
    "        self.al_arr = [] \n",
    "\n",
    "        for _ in range(self.num_learners):\n",
    "            ml = self.weak_learner()\n",
    "            ml.fit(X, y)\n",
    "            y_pred = ml.predict(X) \n",
    "\n",
    "            error = np.mean(np.abs(y_pred - y) / 2 * weights) / np.mean(weights)\n",
    "\n",
    "            if error > 0.5:\n",
    "                break\n",
    "            alpha = self.learning_rate * np.log((1 - error) / error)\n",
    "            self.m_arr.append(ml)\n",
    "            self.al_arr.append(alpha)\n",
    "\n",
    "            weights *= np.exp(-alpha * y * y_pred)\n",
    "            weights /= np.sum(weights)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        for i in range(len(self.m_arr)):\n",
    "            ml = self.m_arr[i]\n",
    "            alpha = self.al_arr[i]\n",
    "            y_pred += alpha * ml.predict(X)\n",
    "\n",
    "        return np.sign(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Ada Boost: 73.18%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = train['Survived'].values\n",
    "X = train.drop('Survived', axis=1).values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6739)\n",
    "\n",
    "Ada_Boost = AdaBoost(weak_learner=DecisionTreeClassifier, num_learners=50, learning_rate=0.1)\n",
    "Ada_Boost.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = Ada_Boost.predict(X_test)\n",
    "\n",
    "Accuracy_AB = accuracy_score(y_test, y_pred)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy for Ada Boost: {:.2f}%'.format(Accuracy_AB*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Decision Tree with Gini: 73.18%\n",
      "Accuracy for Decision Tree with Entropy: 70.95%\n",
      "Accuracy for Decision Tree with Misclassification: 63.69%\n",
      "Accuracy for Random Forest: 79.33%\n",
      "Accuracy for Ada Boost: 73.18%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy for Decision Tree with Gini: {:.2f}%'.format(Accuracy_DT_gini*100))\n",
    "print('Accuracy for Decision Tree with Entropy: {:.2f}%'.format(Accuracy_DT_entropy*100))\n",
    "print('Accuracy for Decision Tree with Misclassification: {:.2f}%'.format(Accuracy_DT_misclassification*100))\n",
    "print(\"Accuracy for Random Forest: {:.2f}%\".format(Accuracy_RF * 100))\n",
    "print(\"Accuracy for Ada Boost: {:.2f}%\".format(Accuracy_AB * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
